{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "If3Wg0Zuo-HY"
   },
   "source": [
    "*Tree-based* methods involve stratifying the feature space into several regions. These models are simple and useful for interpretation, but they don't perform as well as other methods in terms of prediction accuracy. The combination of a large number of trees can improve accuracy at the expense of interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vausSQIntLFX"
   },
   "source": [
    "### **Regression Trees**\n",
    "\n",
    "1. The feature space $J$ is divided into distinct and non-overlapping regions $R_1,R_2,...,R_J$.\n",
    "2. The prediction is the same for every observation that falls within the same region $R_i$: the mean of the response values for the training observations in $R_i$.\n",
    "\n",
    "The regions can have any shape in principle. **By convention, the regions are divided into \"high-dimensional rectangles,\" or *boxes***, for simplicity and interpretation. The goal is to find $R_1,R_2,...,R_J$ that minimize\n",
    "\n",
    "$$RSS=\\sum^J_{J=1} \\sum_{i \\in R_J} (y_i - \\hat{y}_{R_J})^2$$\n",
    "\n",
    "where $\\hat{y}_{R_J}$ is the mean response for the training observations in the $jth$ box.\n",
    "\n",
    "Suppose there are only two features $X_1$ and $X_2$ divided into $7$ regions $R_1,R_2,...,R_7$, then a decision tree may look something like this:\n",
    "\n",
    "![decisionTreeGraphic.png](decisionTreeGraphic.png)\n",
    "\n",
    "In practice, it's inefficient to consider every possible partition of the feature space into $J$ boxes. Instead, we take a \"top-down, greedy approach\" known as recursive binary splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WMWwLBHLfDH"
   },
   "source": [
    "**Recursive Binary Splitting**\n",
    "\n",
    "1. We first select the feature $X_j$ and the cutpoint $s$ such that splitting the feature space into the regions $\\{X|X_j < s\\}$ and $\\{X|X_j \\geq s\\}$ leads to the greatest reduction in $RSS$.\n",
    "\n",
    "  - Considering all features $X_1,X_2,...,X_p$ and all possible cutpoints $s$ for each feature, the feature and cutpoint that results in the lowest $RSS$ is chosen. \n",
    "\n",
    "For any $j$ and $s$, let the **half-planes** be defined as\n",
    "\n",
    "$$R_1(j,s)=\\{X|X_j < s\\}$$ $$and$$ $$R_2(j,s)=\\{X|X_j \\geq s\\}$$\n",
    "\n",
    "Then, we seek the value of $j$ and $s$ that minimizes\n",
    "\n",
    "$$\\sum_{i=x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i=x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2$$\n",
    "\n",
    "2. The process above is repeated as we look for the best feature and cutpoint in order to further split the data for the lowest $RSS$, but this time one of the two previously identified regions is split. Now having three regions in total, we again split one of them into two (and so on). This continues until a **stopping criterion** is reached (i.e., until no region contains more than five observations). \n",
    "\n",
    "**Recursive binary splitting may produce good predictions on the training data, but is likely to suffer from overfitting due to the tree being too complex.** At the expense of slightly greater bias, a smaller tree with fewer regions might lead to a lower variance. \n",
    "\n",
    "*Alternative 1:* Build a tree only so long as the decrease in $RSS$ due to each split exceeds some threshold. This yields smaller trees. However, it may be the case that there is a split later on that leads to a significant reduction in $RSS$. \n",
    "\n",
    "*Alternative 2:* Build a very large tree $T_0$, then prune it back in order to obtain a subtree. This approach is known as tree pruning.\n",
    "\n",
    "Consider *Alternative 2* for creating decision trees by selecting the subtree with the lowest test error rate. Given a subtree, the test error can be estimated using cross-validation. Note that it's too complex to consider all possible subtrees. Instead, through **cost-complexicity tree pruning**, a small set of subtrees indexed by a nonnegative **tuning parameter** $\\alpha$ can be taken into consideration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "StDbHamdqtML"
   },
   "source": [
    "**The Algorithm Behind Building Regression Trees**\n",
    "\n",
    "1. Grow a large tree $T_0$ on the training data using recursive binary splitting, stopping only when each **terminal node** has fewer than some minimum number of observations (the stopping criterion).\n",
    "2. Obtain a sequence of best subtrees as a function of the tuning parameter $\\alpha$ using cost-complexicity tree pruning.\n",
    "3. Choose $\\alpha$ using $K$-fold cross-validation. For each $k=1,2,...K$:\n",
    "\n",
    "  - Repeat Steps $1$ and $2$ on all but the $kth$ fold of the training data.\n",
    "  - Evaluate the mean squared prediction error on the test data in the left-out $kth$ fold as a function of $\\alpha$.\n",
    "\n",
    "Average the results for each value of $\\alpha$ and pick the one that minimizes the average error.\n",
    "\n",
    "4. Choose the subtree from Step $2$ that corresponds to the chosen $\\alpha$.\n",
    "\n",
    "Note that for each $\\alpha$, there corresponds a subtree $T \\subset T_0$ such that\n",
    "\n",
    "$$ \\sum^{|T|}_{m=1} \\sum_{x_i \\in R_m} (y_i - \\hat{y}_{R_1})^2 + \\alpha|T|$$\n",
    "\n",
    "is as small as possible. \n",
    "\n",
    "- $|T|$ is the number of terminal nodes of $T$. \n",
    "- $R_m$ is the rectangle/box (the subset of feature space) that corresponds to the $mth$ terminal node\n",
    "\n",
    "**$\\alpha$ controls the trade-off between the subtree's complexicity and its fit to the training data. $T = T_0$ when $\\alpha=0$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ygfrVKW2mxG"
   },
   "source": [
    "### **Classification Trees**\n",
    "\n",
    "Classification trees are similar to regression trees, except that they're used to predict a qualitative response. \n",
    "\n",
    "- Instead of the mean response as in the case of regression trees, the prediction for a given observation is the most commonly occuring class in the region to which it belongs.\n",
    "- Instead of the $RSS$, recursive binary splitting uses other metrics such as the **classification error rate** $E$, the fraction of the training observations in a given region that don't belong to the most common class within it:\n",
    "\n",
    "$$E = 1 - max_k(\\hat{p}_{mk})$$\n",
    "\n",
    "where $\\hat{p}_{mk}$ is the proportion of training observations in the $mth$ region that are from the $kth$ class. Note that $E$ has been shown to not be \"sufficiently sensitive\" for building classification trees. \n",
    "\n",
    "**There are two preferred alternatives to $E$:**\n",
    "\n",
    "1. The **Gini index** measures the total variance across the $K$ classes. It is a measure of **node purity**. The smaller the Gini index, the more there are observations of the same class within a node.\n",
    "\n",
    "$$G = \\sum^K_{k=1} \\hat{p}_{mk}(1 - \\hat{p}_{mk})$$\n",
    "\n",
    "2. **Entropy**\n",
    "\n",
    "$$D = -\\sum^K_{k=1} \\hat{p}_{mk}log\\hat{p}_{mk}$$\n",
    "\n",
    "$G$ and $D$ turn out to be very similar numerically, and either one can be used for recursive binary splitting. Both are more sensitive to node purity than $E$. **In classification trees, the nodes should be as pure as possible; that is, there are as much of the same classes within each node as possible (node purity). A small Gini index or entropy indicates high node purity.** \n",
    "\n",
    "Lastly, there is no need to create dummy variables for categorical features in both regression and classification trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_skjdmu6vjs"
   },
   "source": [
    "Advantages and Disadvantages of Decision Trees\n",
    "\n",
    "- Easy to explain and closely mirrors human decision making ($+$)\n",
    "- Interpretability and visualization ($+$)\n",
    "- Higher prediction error than other models ($-$)\n",
    "- Non-robust and unstable to small changes in the data ($-$)\n",
    "\n",
    "**At the expense of interpretability, there are three methods that can significantly improve the prediction accuracy of decision trees:**\n",
    "\n",
    "1. bagging\n",
    "2. boosting\n",
    "3. random forests\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNzuDDm/2PatZ//RMN+9ewz",
   "collapsed_sections": [],
   "mount_file_id": "1nnLvYhyFG3gGe4Ej6GN2e84pju-gb4oI",
   "name": "Decision Trees.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
