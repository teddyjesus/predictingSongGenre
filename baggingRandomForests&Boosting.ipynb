{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baggingRandomForests&Boosting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEhUWzvKDmKqSUC1iaxQP/",
      "include_colab_link": false
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teddyjesus/treeBasedMethods/blob/master/baggingRandomForests%26Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAmjVHcAsprp",
        "colab_type": "text"
      },
      "source": [
        "### **Bootstrap Aggregation / Bagging**\n",
        "\n",
        "Recall that decision trees suffer from high variance. Bagging is a general procedure for reducing the variance of a machine learning algorithm. Repeated samples from the training data are taken to yield $B$ sets, then the model is trained on the $bth$ bootstrapped set to get $\\hat{f}^{*b}(x)$. The average of the predictions is given by \n",
        "\n",
        "$$\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum^B_{b=1}\\hat{f}^{*b}(x)$$\n",
        "\n",
        "**Bagged Regression Trees**\n",
        "\n",
        "$B$ trees are constructed from $B$ bootstrapped training sets and the resulting predictions are averaged. The trees are grown deep without pruning, such that each individual tree has high variance but low bias. The variance is reduced by taking the average of the $B$ trees.\n",
        "\n",
        "**Bagged Classification Trees**\n",
        "\n",
        "For a given test observation, the class predicted by each of the $B$ trees is recorded and the **majority vote** is taken: the prediction is the most common class among the $B$ predictions.\n",
        "\n",
        "In general, although the test error rate is a function of $B$, **the size of $B$ is not a critical parameter in bagging**; a large $B$ doesn't lead to overfitting. $B$ must be sufficiently large for the error to \"settle down.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ffYzA4KyCdL",
        "colab_type": "text"
      },
      "source": [
        "**Out-of-Bag Estimation**\n",
        "\n",
        "Each bagged tree has been shown to only use around two-thirds of the training data. The remaining one-third of observations is known as the out-of-bag ($OOB$) observations. \n",
        "\n",
        "Thus, the $ith$ observation can be predicted by the bagged trees where it was $OOB$ to yield $B/3$ predictions for that observation. And to obtain a single prediction for the $ith$ observation, the predictions can be averaged (for regression trees) or the majority vote can be taken (for classification trees). By taking $OOB$ predictions for all the observations, an $MSE$ or classification error can be computed. **This method is equivalent to $LOOCV$**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEMhkJxS0ml-",
        "colab_type": "text"
      },
      "source": [
        "### **Random Forests**\n",
        "\n",
        "Just like bagging, several decision trees are constructed from bootstrapped training sets. When building these trees, **each time a split in a tree is considered, a random sample of $m$ features is chosen as split candidates from the full set of $p$ features**. "
      ]
    }
  ]
}
