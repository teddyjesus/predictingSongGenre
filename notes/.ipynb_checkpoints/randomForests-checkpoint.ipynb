{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forests**\n",
    "\n",
    "Random forests start off the same way as bagging: a number of decision trees are created from bootstrapped training sets. When creating the individual trees, a random subset of $m$ features out of all $p$ features is considered at each split. By convention, $m \\approx \\sqrt{p}$. When $m=p$, a random forest is the same as \"traditional\" bagging. In general, a small $m$ is helpful when many of the features are correlated. \n",
    "\n",
    "**How are random forests an improvement to \"traditional\" bagging?**\n",
    "\n",
    "In addition to other predictors in the data, suppose there is a particularly \"strong\" feature $f$ that contains the most information about the outcome. Then, most of the $B$ trees created from $B$ bootstrapped training sets must contain $f$ at the top split. This leads to high correlation between the bagged trees, such that the reduction in variance over one decision tree (the goal of bagging) may not be substantial. By forcing each split to consider only a random subset of the features, it can be shown that $f$ will not be considered on $\\frac{m-p}{p}$ of the splits on average. **Random forests essentially decorrelate the bagged trees to make more accurate predictions**. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
