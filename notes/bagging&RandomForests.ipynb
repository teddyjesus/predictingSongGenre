{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bagging**\n",
    "\n",
    "Bootstrap aggregation, or bagging, is a general procedure for reducing the variance of a statistical learning model. Recall that the variance of the sum of $n$ independent observations $Z_1,Z_2,...,Z_n$ each with variance $\\sigma^2$ is $\\frac{\\sigma^2}{n}$\n",
    "\n",
    "Note that $Var(Z_i)>\\frac{\\sigma^2}{n}$ for $i=1,2,...,n$. **Within the context of machine learning, bagging is the repeated sampling with replacement from the training data to generate $B$ bootstrapped training sets.** The model is trained on each bootstrapped set $b \\in B$ to yield predictions $\\hat {f}^{*b}(x)$. Then, the $B$ resulting predictions are averaged to yield\n",
    "\n",
    "$$\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum^B_{b=1}\\hat{f}^{*b}(x)$$\n",
    "\n",
    "**Bagged Regression Trees**\n",
    "\n",
    "$B$ trees are constructed from $B$ bootstrapped training sets and the resulting predictions are averaged. The trees are grown deep without pruning, such that each individual tree has high variance but low bias. The variance is reduced by taking the average of the $B$ trees.\n",
    "\n",
    "**Bagged Classification Trees**\n",
    "\n",
    "For a given test observation, the class predicted by each of the $B$ trees is recorded and the **majority vote** is taken: the prediction is the most common class among the $B$ predictions. \n",
    "\n",
    "In general, although the test error rate is a function of $B$, **the size of $B$ is not a critical parameter in bagging**; a large $B$ doesn't lead to overfitting. $B$ must be sufficiently large for the error to \"settle down.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Out-of-Bag Estimation**\n",
    "\n",
    "Each bagged tree has been shown to only use around two-thirds of the training data. The remaining one-third of observations is known as the out-of-bag ($OOB$) observations. \n",
    "\n",
    "Thus, the $ith$ observation can be predicted by the bagged trees where it was $OOB$ to yield $B/3$ predictions for that observation. And to obtain a single prediction for the $ith$ observation, the predictions can be averaged (for regression trees) or the majority vote can be taken (for classification trees). By taking $OOB$ predictions for all the observations, an $MSE$ or classification error can be computed. This method is approximately equivalent to $LOOCV$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forests**\n",
    "\n",
    "Random forests start off the same way as bagging: a number of decision trees are created from bootstrapped training sets. When creating the individual trees, a random subset of $m$ features out of all $p$ features is considered at each split. By convention, $m \\approx \\sqrt{p}$. When $m=p$, a random forest is the same as \"traditional\" bagging. In general, a small $m$ is helpful when many of the features are correlated. \n",
    "\n",
    "**How are random forests an improvement to \"traditional\" bagging?**\n",
    "\n",
    "In addition to other predictors in the data, suppose there is a particularly \"strong\" feature $f$ that contains the most information about the outcome. Then, most of the $B$ trees created from $B$ bootstrapped training sets must contain $f$ at the top split. This leads to high correlation between the bagged trees, such that the reduction in variance over one decision tree (the goal of bagging) may not be substantial. By forcing each split to consider only a random subset of the features, it can be shown that $f$ will not be considered on $\\frac{m-p}{p}$ of the splits on average. **Random forests essentially decorrelate the bagged trees to make more accurate predictions**. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
